{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOxuk+h1VArKfhU73ufVTBh"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tA3KBIoEXyLp","executionInfo":{"status":"ok","timestamp":1745509314158,"user_tz":-330,"elapsed":39095,"user":{"displayName":"it21264634 Sujitha.S","userId":"04194498455262512245"}},"outputId":"81559340-45d2-45cb-d733-27d75a59f647"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n"]},{"cell_type":"code","source":["!git clone https://github.com/ggerganov/llama.cpp\n","%cd llama.cpp\n","!pip install -r requirements.txt\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"u0OzPEWRYCT0","executionInfo":{"status":"ok","timestamp":1745509420417,"user_tz":-330,"elapsed":103356,"user":{"displayName":"it21264634 Sujitha.S","userId":"04194498455262512245"}},"outputId":"4b4b4ccd-dfd5-4648-a3cc-e34470f13252"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'llama.cpp'...\n","remote: Enumerating objects: 49145, done.\u001b[K\n","remote: Counting objects: 100% (251/251), done.\u001b[K\n","remote: Compressing objects: 100% (162/162), done.\u001b[K\n","remote: Total 49145 (delta 176), reused 89 (delta 89), pack-reused 48894 (from 3)\u001b[K\n","Receiving objects: 100% (49145/49145), 103.48 MiB | 23.84 MiB/s, done.\n","Resolving deltas: 100% (35362/35362), done.\n","/content/llama.cpp\n","Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cpu, https://download.pytorch.org/whl/cpu, https://download.pytorch.org/whl/cpu, https://download.pytorch.org/whl/cpu\n","Collecting numpy~=1.26.4 (from -r ./requirements/requirements-convert_legacy_llama.txt (line 1))\n","  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: sentencepiece~=0.2.0 in /usr/local/lib/python3.11/dist-packages (from -r ./requirements/requirements-convert_legacy_llama.txt (line 2)) (0.2.0)\n","Requirement already satisfied: transformers<5.0.0,>=4.45.1 in /usr/local/lib/python3.11/dist-packages (from -r ./requirements/requirements-convert_legacy_llama.txt (line 3)) (4.51.3)\n","Collecting gguf>=0.1.0 (from -r ./requirements/requirements-convert_legacy_llama.txt (line 4))\n","  Downloading gguf-0.16.2-py3-none-any.whl.metadata (4.4 kB)\n","Collecting protobuf<5.0.0,>=4.21.0 (from -r ./requirements/requirements-convert_legacy_llama.txt (line 5))\n","  Downloading protobuf-4.25.7-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n","Collecting torch~=2.2.1 (from -r ./requirements/requirements-convert_hf_to_gguf.txt (line 3))\n","  Downloading https://download.pytorch.org/whl/cpu/torch-2.2.2%2Bcpu-cp311-cp311-linux_x86_64.whl (186.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m186.8/186.8 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting aiohttp~=3.9.3 (from -r ./requirements/requirements-tool_bench.txt (line 1))\n","  Downloading aiohttp-3.9.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.5 kB)\n","Requirement already satisfied: pytest~=8.3.3 in /usr/local/lib/python3.11/dist-packages (from -r ./requirements/requirements-tool_bench.txt (line 2)) (8.3.5)\n","Collecting huggingface_hub~=0.23.2 (from -r ./requirements/requirements-tool_bench.txt (line 3))\n","  Downloading huggingface_hub-0.23.5-py3-none-any.whl.metadata (12 kB)\n","Requirement already satisfied: matplotlib~=3.10.0 in /usr/local/lib/python3.11/dist-packages (from -r ./requirements/requirements-tool_bench.txt (line 4)) (3.10.0)\n","Collecting openai~=1.55.3 (from -r ./requirements/requirements-tool_bench.txt (line 6))\n","  Downloading openai-1.55.3-py3-none-any.whl.metadata (24 kB)\n","Collecting pandas~=2.2.3 (from -r ./requirements/requirements-tool_bench.txt (line 7))\n","  Downloading pandas-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.9/89.9 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting prometheus-client~=0.20.0 (from -r ./requirements/requirements-tool_bench.txt (line 8))\n","  Downloading prometheus_client-0.20.0-py3-none-any.whl.metadata (1.8 kB)\n","Requirement already satisfied: requests~=2.32.3 in /usr/local/lib/python3.11/dist-packages (from -r ./requirements/requirements-tool_bench.txt (line 9)) (2.32.3)\n","Collecting wget~=3.2 (from -r ./requirements/requirements-tool_bench.txt (line 10))\n","  Downloading wget-3.2.zip (10 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: typer~=0.15.1 in /usr/local/lib/python3.11/dist-packages (from -r ./requirements/requirements-tool_bench.txt (line 11)) (0.15.2)\n","Requirement already satisfied: seaborn~=0.13.2 in /usr/local/lib/python3.11/dist-packages (from -r ./requirements/requirements-tool_bench.txt (line 12)) (0.13.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.45.1->-r ./requirements/requirements-convert_legacy_llama.txt (line 3)) (3.18.0)\n","INFO: pip is looking at multiple versions of transformers to determine which version is compatible with other requirements. This could take a while.\n","Collecting transformers<5.0.0,>=4.45.1 (from -r ./requirements/requirements-convert_legacy_llama.txt (line 3))\n","  Downloading transformers-4.51.2-py3-none-any.whl.metadata (38 kB)\n","  Downloading transformers-4.51.1-py3-none-any.whl.metadata (38 kB)\n","  Downloading transformers-4.51.0-py3-none-any.whl.metadata (38 kB)\n","  Downloading transformers-4.50.3-py3-none-any.whl.metadata (39 kB)\n","  Downloading transformers-4.50.2-py3-none-any.whl.metadata (39 kB)\n","  Downloading transformers-4.50.1-py3-none-any.whl.metadata (39 kB)\n","  Downloading transformers-4.50.0-py3-none-any.whl.metadata (39 kB)\n","INFO: pip is still looking at multiple versions of transformers to determine which version is compatible with other requirements. This could take a while.\n","  Downloading transformers-4.49.0-py3-none-any.whl.metadata (44 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Downloading transformers-4.48.3-py3-none-any.whl.metadata (44 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Downloading transformers-4.48.2-py3-none-any.whl.metadata (44 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Downloading transformers-4.48.1-py3-none-any.whl.metadata (44 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Downloading transformers-4.48.0-py3-none-any.whl.metadata (44 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hINFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n","  Downloading transformers-4.47.1-py3-none-any.whl.metadata (44 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.1/44.1 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Downloading transformers-4.47.0-py3-none-any.whl.metadata (43 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.5/43.5 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Downloading transformers-4.46.3-py3-none-any.whl.metadata (44 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.1/44.1 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.45.1->-r ./requirements/requirements-convert_legacy_llama.txt (line 3)) (24.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.45.1->-r ./requirements/requirements-convert_legacy_llama.txt (line 3)) (6.0.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.45.1->-r ./requirements/requirements-convert_legacy_llama.txt (line 3)) (2024.11.6)\n","Collecting tokenizers<0.21,>=0.20 (from transformers<5.0.0,>=4.45.1->-r ./requirements/requirements-convert_legacy_llama.txt (line 3))\n","  Downloading tokenizers-0.20.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.45.1->-r ./requirements/requirements-convert_legacy_llama.txt (line 3)) (0.5.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.45.1->-r ./requirements/requirements-convert_legacy_llama.txt (line 3)) (4.67.1)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch~=2.2.1->-r ./requirements/requirements-convert_hf_to_gguf.txt (line 3)) (4.13.2)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch~=2.2.1->-r ./requirements/requirements-convert_hf_to_gguf.txt (line 3)) (1.13.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch~=2.2.1->-r ./requirements/requirements-convert_hf_to_gguf.txt (line 3)) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch~=2.2.1->-r ./requirements/requirements-convert_hf_to_gguf.txt (line 3)) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch~=2.2.1->-r ./requirements/requirements-convert_hf_to_gguf.txt (line 3)) (2025.3.2)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp~=3.9.3->-r ./requirements/requirements-tool_bench.txt (line 1)) (1.3.2)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp~=3.9.3->-r ./requirements/requirements-tool_bench.txt (line 1)) (25.3.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp~=3.9.3->-r ./requirements/requirements-tool_bench.txt (line 1)) (1.5.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp~=3.9.3->-r ./requirements/requirements-tool_bench.txt (line 1)) (6.4.3)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp~=3.9.3->-r ./requirements/requirements-tool_bench.txt (line 1)) (1.19.0)\n","Requirement already satisfied: iniconfig in /usr/local/lib/python3.11/dist-packages (from pytest~=8.3.3->-r ./requirements/requirements-tool_bench.txt (line 2)) (2.1.0)\n","Requirement already satisfied: pluggy<2,>=1.5 in /usr/local/lib/python3.11/dist-packages (from pytest~=8.3.3->-r ./requirements/requirements-tool_bench.txt (line 2)) (1.5.0)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib~=3.10.0->-r ./requirements/requirements-tool_bench.txt (line 4)) (1.3.2)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib~=3.10.0->-r ./requirements/requirements-tool_bench.txt (line 4)) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib~=3.10.0->-r ./requirements/requirements-tool_bench.txt (line 4)) (4.57.0)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib~=3.10.0->-r ./requirements/requirements-tool_bench.txt (line 4)) (1.4.8)\n","Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib~=3.10.0->-r ./requirements/requirements-tool_bench.txt (line 4)) (11.1.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib~=3.10.0->-r ./requirements/requirements-tool_bench.txt (line 4)) (3.2.3)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib~=3.10.0->-r ./requirements/requirements-tool_bench.txt (line 4)) (2.8.2)\n","Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai~=1.55.3->-r ./requirements/requirements-tool_bench.txt (line 6)) (4.9.0)\n","Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai~=1.55.3->-r ./requirements/requirements-tool_bench.txt (line 6)) (1.9.0)\n","Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai~=1.55.3->-r ./requirements/requirements-tool_bench.txt (line 6)) (0.28.1)\n","Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai~=1.55.3->-r ./requirements/requirements-tool_bench.txt (line 6)) (0.9.0)\n","Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from openai~=1.55.3->-r ./requirements/requirements-tool_bench.txt (line 6)) (2.11.3)\n","Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai~=1.55.3->-r ./requirements/requirements-tool_bench.txt (line 6)) (1.3.1)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas~=2.2.3->-r ./requirements/requirements-tool_bench.txt (line 7)) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas~=2.2.3->-r ./requirements/requirements-tool_bench.txt (line 7)) (2025.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests~=2.32.3->-r ./requirements/requirements-tool_bench.txt (line 9)) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests~=2.32.3->-r ./requirements/requirements-tool_bench.txt (line 9)) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests~=2.32.3->-r ./requirements/requirements-tool_bench.txt (line 9)) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests~=2.32.3->-r ./requirements/requirements-tool_bench.txt (line 9)) (2025.1.31)\n","Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer~=0.15.1->-r ./requirements/requirements-tool_bench.txt (line 11)) (8.1.8)\n","Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer~=0.15.1->-r ./requirements/requirements-tool_bench.txt (line 11)) (1.5.4)\n","Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer~=0.15.1->-r ./requirements/requirements-tool_bench.txt (line 11)) (13.9.4)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai~=1.55.3->-r ./requirements/requirements-tool_bench.txt (line 6)) (1.0.8)\n","Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai~=1.55.3->-r ./requirements/requirements-tool_bench.txt (line 6)) (0.14.0)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai~=1.55.3->-r ./requirements/requirements-tool_bench.txt (line 6)) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai~=1.55.3->-r ./requirements/requirements-tool_bench.txt (line 6)) (2.33.1)\n","Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai~=1.55.3->-r ./requirements/requirements-tool_bench.txt (line 6)) (0.4.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib~=3.10.0->-r ./requirements/requirements-tool_bench.txt (line 4)) (1.17.0)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer~=0.15.1->-r ./requirements/requirements-tool_bench.txt (line 11)) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer~=0.15.1->-r ./requirements/requirements-tool_bench.txt (line 11)) (2.18.0)\n","Requirement already satisfied: propcache>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from yarl<2.0,>=1.0->aiohttp~=3.9.3->-r ./requirements/requirements-tool_bench.txt (line 1)) (0.3.1)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch~=2.2.1->-r ./requirements/requirements-convert_hf_to_gguf.txt (line 3)) (3.0.2)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch~=2.2.1->-r ./requirements/requirements-convert_hf_to_gguf.txt (line 3)) (1.3.0)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer~=0.15.1->-r ./requirements/requirements-tool_bench.txt (line 11)) (0.1.2)\n","Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m69.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading transformers-4.46.3-py3-none-any.whl (10.0 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m92.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading gguf-0.16.2-py3-none-any.whl (92 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.2/92.2 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading protobuf-4.25.7-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.6/294.6 kB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading aiohttp-3.9.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m58.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading huggingface_hub-0.23.5-py3-none-any.whl (402 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m402.8/402.8 kB\u001b[0m \u001b[31m30.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading openai-1.55.3-py3-none-any.whl (389 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m389.6/389.6 kB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pandas-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m82.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading prometheus_client-0.20.0-py3-none-any.whl (54 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading tokenizers-0.20.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m72.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hBuilding wheels for collected packages: wget\n","  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9655 sha256=471c4c63d9ea031f2718aab67fe0b086883caa426d2eaa0439ea4c6dc1330f28\n","  Stored in directory: /root/.cache/pip/wheels/40/b3/0f/a40dbd1c6861731779f62cc4babcb234387e11d697df70ee97\n","Successfully built wget\n","Installing collected packages: wget, protobuf, prometheus-client, numpy, torch, pandas, huggingface_hub, gguf, aiohttp, tokenizers, openai, transformers\n","  Attempting uninstall: protobuf\n","    Found existing installation: protobuf 5.29.4\n","    Uninstalling protobuf-5.29.4:\n","      Successfully uninstalled protobuf-5.29.4\n","  Attempting uninstall: prometheus-client\n","    Found existing installation: prometheus_client 0.21.1\n","    Uninstalling prometheus_client-0.21.1:\n","      Successfully uninstalled prometheus_client-0.21.1\n","  Attempting uninstall: numpy\n","    Found existing installation: numpy 2.0.2\n","    Uninstalling numpy-2.0.2:\n","      Successfully uninstalled numpy-2.0.2\n","  Attempting uninstall: torch\n","    Found existing installation: torch 2.6.0+cu124\n","    Uninstalling torch-2.6.0+cu124:\n","      Successfully uninstalled torch-2.6.0+cu124\n","  Attempting uninstall: pandas\n","    Found existing installation: pandas 2.2.2\n","    Uninstalling pandas-2.2.2:\n","      Successfully uninstalled pandas-2.2.2\n","  Attempting uninstall: huggingface_hub\n","    Found existing installation: huggingface-hub 0.30.2\n","    Uninstalling huggingface-hub-0.30.2:\n","      Successfully uninstalled huggingface-hub-0.30.2\n","  Attempting uninstall: aiohttp\n","    Found existing installation: aiohttp 3.11.15\n","    Uninstalling aiohttp-3.11.15:\n","      Successfully uninstalled aiohttp-3.11.15\n","  Attempting uninstall: tokenizers\n","    Found existing installation: tokenizers 0.21.1\n","    Uninstalling tokenizers-0.21.1:\n","      Successfully uninstalled tokenizers-0.21.1\n","  Attempting uninstall: openai\n","    Found existing installation: openai 1.75.0\n","    Uninstalling openai-1.75.0:\n","      Successfully uninstalled openai-1.75.0\n","  Attempting uninstall: transformers\n","    Found existing installation: transformers 4.51.3\n","    Uninstalling transformers-4.51.3:\n","      Successfully uninstalled transformers-4.51.3\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\n","torchaudio 2.6.0+cu124 requires torch==2.6.0, but you have torch 2.2.2+cpu which is incompatible.\n","ydf 0.11.0 requires protobuf<6.0.0,>=5.29.1, but you have protobuf 4.25.7 which is incompatible.\n","thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n","grpcio-status 1.71.0 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 4.25.7 which is incompatible.\n","torchvision 0.21.0+cu124 requires torch==2.6.0, but you have torch 2.2.2+cpu which is incompatible.\n","peft 0.14.0 requires huggingface-hub>=0.25.0, but you have huggingface-hub 0.23.5 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed aiohttp-3.9.5 gguf-0.16.2 huggingface_hub-0.23.5 numpy-1.26.4 openai-1.55.3 pandas-2.2.3 prometheus-client-0.20.0 protobuf-4.25.7 tokenizers-0.20.3 torch-2.2.2+cpu transformers-4.46.3 wget-3.2\n"]}]},{"cell_type":"code","source":["%cd /content/llama.cpp\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UwNheQjqYEIM","executionInfo":{"status":"ok","timestamp":1745509785503,"user_tz":-330,"elapsed":37,"user":{"displayName":"it21264634 Sujitha.S","userId":"04194498455262512245"}},"outputId":"526d6958-2c29-4b1c-84b8-67571b96393d"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/llama.cpp\n"]}]},{"cell_type":"code","source":["%cd /content/llama.cpp\n","\n","MODEL_DIR = \"/content/drive/MyDrive/Research Project/saved_model\"\n","\n","!python3 convert_hf_to_gguf.py \\\n","  --outtype q8_0 \\\n","  --outfile /content/llama2-q8_0.gguf \\\n","  \"$MODEL_DIR\"\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-oyeYjlXfXEb","executionInfo":{"status":"ok","timestamp":1745510072403,"user_tz":-330,"elapsed":117624,"user":{"displayName":"it21264634 Sujitha.S","userId":"04194498455262512245"}},"outputId":"522cfd48-e58e-4255-cb02-8ec33372bbf8"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/llama.cpp\n","INFO:hf-to-gguf:Loading model: saved_model\n","INFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only\n","INFO:hf-to-gguf:Exporting model...\n","INFO:hf-to-gguf:rope_freqs.weight,           torch.float32 --> F32, shape = {32}\n","INFO:hf-to-gguf:gguf: loading model part 'model.safetensors'\n","INFO:hf-to-gguf:token_embd.weight,           torch.float32 --> Q8_0, shape = {2048, 128256}\n","INFO:hf-to-gguf:blk.0.attn_norm.weight,      torch.float32 --> F32, shape = {2048}\n","INFO:hf-to-gguf:blk.0.ffn_down.weight,       torch.float32 --> Q8_0, shape = {8192, 2048}\n","INFO:hf-to-gguf:blk.0.ffn_gate.weight,       torch.float32 --> Q8_0, shape = {2048, 8192}\n","INFO:hf-to-gguf:blk.0.ffn_up.weight,         torch.float32 --> Q8_0, shape = {2048, 8192}\n","INFO:hf-to-gguf:blk.0.ffn_norm.weight,       torch.float32 --> F32, shape = {2048}\n","INFO:hf-to-gguf:blk.0.attn_k.weight,         torch.float32 --> Q8_0, shape = {2048, 512}\n","INFO:hf-to-gguf:blk.0.attn_output.weight,    torch.float32 --> Q8_0, shape = {2048, 2048}\n","INFO:hf-to-gguf:blk.0.attn_q.weight,         torch.float32 --> Q8_0, shape = {2048, 2048}\n","INFO:hf-to-gguf:blk.0.attn_v.weight,         torch.float32 --> Q8_0, shape = {2048, 512}\n","INFO:hf-to-gguf:blk.1.attn_norm.weight,      torch.float32 --> F32, shape = {2048}\n","INFO:hf-to-gguf:blk.1.ffn_down.weight,       torch.float32 --> Q8_0, shape = {8192, 2048}\n","INFO:hf-to-gguf:blk.1.ffn_gate.weight,       torch.float32 --> Q8_0, shape = {2048, 8192}\n","INFO:hf-to-gguf:blk.1.ffn_up.weight,         torch.float32 --> Q8_0, shape = {2048, 8192}\n","INFO:hf-to-gguf:blk.1.ffn_norm.weight,       torch.float32 --> F32, shape = {2048}\n","INFO:hf-to-gguf:blk.1.attn_k.weight,         torch.float32 --> Q8_0, shape = {2048, 512}\n","INFO:hf-to-gguf:blk.1.attn_output.weight,    torch.float32 --> Q8_0, shape = {2048, 2048}\n","INFO:hf-to-gguf:blk.1.attn_q.weight,         torch.float32 --> Q8_0, shape = {2048, 2048}\n","INFO:hf-to-gguf:blk.1.attn_v.weight,         torch.float32 --> Q8_0, shape = {2048, 512}\n","INFO:hf-to-gguf:blk.10.attn_norm.weight,     torch.float32 --> F32, shape = {2048}\n","INFO:hf-to-gguf:blk.10.ffn_down.weight,      torch.float32 --> Q8_0, shape = {8192, 2048}\n","INFO:hf-to-gguf:blk.10.ffn_gate.weight,      torch.float32 --> Q8_0, shape = {2048, 8192}\n","INFO:hf-to-gguf:blk.10.ffn_up.weight,        torch.float32 --> Q8_0, shape = {2048, 8192}\n","INFO:hf-to-gguf:blk.10.ffn_norm.weight,      torch.float32 --> F32, shape = {2048}\n","INFO:hf-to-gguf:blk.10.attn_k.weight,        torch.float32 --> Q8_0, shape = {2048, 512}\n","INFO:hf-to-gguf:blk.10.attn_output.weight,   torch.float32 --> Q8_0, shape = {2048, 2048}\n","INFO:hf-to-gguf:blk.10.attn_q.weight,        torch.float32 --> Q8_0, shape = {2048, 2048}\n","INFO:hf-to-gguf:blk.10.attn_v.weight,        torch.float32 --> Q8_0, shape = {2048, 512}\n","INFO:hf-to-gguf:blk.11.attn_norm.weight,     torch.float32 --> F32, shape = {2048}\n","INFO:hf-to-gguf:blk.11.ffn_down.weight,      torch.float32 --> Q8_0, shape = {8192, 2048}\n","INFO:hf-to-gguf:blk.11.ffn_gate.weight,      torch.float32 --> Q8_0, shape = {2048, 8192}\n","INFO:hf-to-gguf:blk.11.ffn_up.weight,        torch.float32 --> Q8_0, shape = {2048, 8192}\n","INFO:hf-to-gguf:blk.11.ffn_norm.weight,      torch.float32 --> F32, shape = {2048}\n","INFO:hf-to-gguf:blk.11.attn_k.weight,        torch.float32 --> Q8_0, shape = {2048, 512}\n","INFO:hf-to-gguf:blk.11.attn_output.weight,   torch.float32 --> Q8_0, shape = {2048, 2048}\n","INFO:hf-to-gguf:blk.11.attn_q.weight,        torch.float32 --> Q8_0, shape = {2048, 2048}\n","INFO:hf-to-gguf:blk.11.attn_v.weight,        torch.float32 --> Q8_0, shape = {2048, 512}\n","INFO:hf-to-gguf:blk.12.attn_norm.weight,     torch.float32 --> F32, shape = {2048}\n","INFO:hf-to-gguf:blk.12.ffn_down.weight,      torch.float32 --> Q8_0, shape = {8192, 2048}\n","INFO:hf-to-gguf:blk.12.ffn_gate.weight,      torch.float32 --> Q8_0, shape = {2048, 8192}\n","INFO:hf-to-gguf:blk.12.ffn_up.weight,        torch.float32 --> Q8_0, shape = {2048, 8192}\n","INFO:hf-to-gguf:blk.12.ffn_norm.weight,      torch.float32 --> F32, shape = {2048}\n","INFO:hf-to-gguf:blk.12.attn_k.weight,        torch.float32 --> Q8_0, shape = {2048, 512}\n","INFO:hf-to-gguf:blk.12.attn_output.weight,   torch.float32 --> Q8_0, shape = {2048, 2048}\n","INFO:hf-to-gguf:blk.12.attn_q.weight,        torch.float32 --> Q8_0, shape = {2048, 2048}\n","INFO:hf-to-gguf:blk.12.attn_v.weight,        torch.float32 --> Q8_0, shape = {2048, 512}\n","INFO:hf-to-gguf:blk.13.attn_norm.weight,     torch.float32 --> F32, shape = {2048}\n","INFO:hf-to-gguf:blk.13.ffn_down.weight,      torch.float32 --> Q8_0, shape = {8192, 2048}\n","INFO:hf-to-gguf:blk.13.ffn_gate.weight,      torch.float32 --> Q8_0, shape = {2048, 8192}\n","INFO:hf-to-gguf:blk.13.ffn_up.weight,        torch.float32 --> Q8_0, shape = {2048, 8192}\n","INFO:hf-to-gguf:blk.13.ffn_norm.weight,      torch.float32 --> F32, shape = {2048}\n","INFO:hf-to-gguf:blk.13.attn_k.weight,        torch.float32 --> Q8_0, shape = {2048, 512}\n","INFO:hf-to-gguf:blk.13.attn_output.weight,   torch.float32 --> Q8_0, shape = {2048, 2048}\n","INFO:hf-to-gguf:blk.13.attn_q.weight,        torch.float32 --> Q8_0, shape = {2048, 2048}\n","INFO:hf-to-gguf:blk.13.attn_v.weight,        torch.float32 --> Q8_0, shape = {2048, 512}\n","INFO:hf-to-gguf:blk.14.attn_norm.weight,     torch.float32 --> F32, shape = {2048}\n","INFO:hf-to-gguf:blk.14.ffn_down.weight,      torch.float32 --> Q8_0, shape = {8192, 2048}\n","INFO:hf-to-gguf:blk.14.ffn_gate.weight,      torch.float32 --> Q8_0, shape = {2048, 8192}\n","INFO:hf-to-gguf:blk.14.ffn_up.weight,        torch.float32 --> Q8_0, shape = {2048, 8192}\n","INFO:hf-to-gguf:blk.14.ffn_norm.weight,      torch.float32 --> F32, shape = {2048}\n","INFO:hf-to-gguf:blk.14.attn_k.weight,        torch.float32 --> Q8_0, shape = {2048, 512}\n","INFO:hf-to-gguf:blk.14.attn_output.weight,   torch.float32 --> Q8_0, shape = {2048, 2048}\n","INFO:hf-to-gguf:blk.14.attn_q.weight,        torch.float32 --> Q8_0, shape = {2048, 2048}\n","INFO:hf-to-gguf:blk.14.attn_v.weight,        torch.float32 --> Q8_0, shape = {2048, 512}\n","INFO:hf-to-gguf:blk.15.attn_norm.weight,     torch.float32 --> F32, shape = {2048}\n","INFO:hf-to-gguf:blk.15.ffn_down.weight,      torch.float32 --> Q8_0, shape = {8192, 2048}\n","INFO:hf-to-gguf:blk.15.ffn_gate.weight,      torch.float32 --> Q8_0, shape = {2048, 8192}\n","INFO:hf-to-gguf:blk.15.ffn_up.weight,        torch.float32 --> Q8_0, shape = {2048, 8192}\n","INFO:hf-to-gguf:blk.15.ffn_norm.weight,      torch.float32 --> F32, shape = {2048}\n","INFO:hf-to-gguf:blk.15.attn_k.weight,        torch.float32 --> Q8_0, shape = {2048, 512}\n","INFO:hf-to-gguf:blk.15.attn_output.weight,   torch.float32 --> Q8_0, shape = {2048, 2048}\n","INFO:hf-to-gguf:blk.15.attn_q.weight,        torch.float32 --> Q8_0, shape = {2048, 2048}\n","INFO:hf-to-gguf:blk.15.attn_v.weight,        torch.float32 --> Q8_0, shape = {2048, 512}\n","INFO:hf-to-gguf:blk.2.attn_norm.weight,      torch.float32 --> F32, shape = {2048}\n","INFO:hf-to-gguf:blk.2.ffn_down.weight,       torch.float32 --> Q8_0, shape = {8192, 2048}\n","INFO:hf-to-gguf:blk.2.ffn_gate.weight,       torch.float32 --> Q8_0, shape = {2048, 8192}\n","INFO:hf-to-gguf:blk.2.ffn_up.weight,         torch.float32 --> Q8_0, shape = {2048, 8192}\n","INFO:hf-to-gguf:blk.2.ffn_norm.weight,       torch.float32 --> F32, shape = {2048}\n","INFO:hf-to-gguf:blk.2.attn_k.weight,         torch.float32 --> Q8_0, shape = {2048, 512}\n","INFO:hf-to-gguf:blk.2.attn_output.weight,    torch.float32 --> Q8_0, shape = {2048, 2048}\n","INFO:hf-to-gguf:blk.2.attn_q.weight,         torch.float32 --> Q8_0, shape = {2048, 2048}\n","INFO:hf-to-gguf:blk.2.attn_v.weight,         torch.float32 --> Q8_0, shape = {2048, 512}\n","INFO:hf-to-gguf:blk.3.attn_norm.weight,      torch.float32 --> F32, shape = {2048}\n","INFO:hf-to-gguf:blk.3.ffn_down.weight,       torch.float32 --> Q8_0, shape = {8192, 2048}\n","INFO:hf-to-gguf:blk.3.ffn_gate.weight,       torch.float32 --> Q8_0, shape = {2048, 8192}\n","INFO:hf-to-gguf:blk.3.ffn_up.weight,         torch.float32 --> Q8_0, shape = {2048, 8192}\n","INFO:hf-to-gguf:blk.3.ffn_norm.weight,       torch.float32 --> F32, shape = {2048}\n","INFO:hf-to-gguf:blk.3.attn_k.weight,         torch.float32 --> Q8_0, shape = {2048, 512}\n","INFO:hf-to-gguf:blk.3.attn_output.weight,    torch.float32 --> Q8_0, shape = {2048, 2048}\n","INFO:hf-to-gguf:blk.3.attn_q.weight,         torch.float32 --> Q8_0, shape = {2048, 2048}\n","INFO:hf-to-gguf:blk.3.attn_v.weight,         torch.float32 --> Q8_0, shape = {2048, 512}\n","INFO:hf-to-gguf:blk.4.attn_norm.weight,      torch.float32 --> F32, shape = {2048}\n","INFO:hf-to-gguf:blk.4.ffn_down.weight,       torch.float32 --> Q8_0, shape = {8192, 2048}\n","INFO:hf-to-gguf:blk.4.ffn_gate.weight,       torch.float32 --> Q8_0, shape = {2048, 8192}\n","INFO:hf-to-gguf:blk.4.ffn_up.weight,         torch.float32 --> Q8_0, shape = {2048, 8192}\n","INFO:hf-to-gguf:blk.4.ffn_norm.weight,       torch.float32 --> F32, shape = {2048}\n","INFO:hf-to-gguf:blk.4.attn_k.weight,         torch.float32 --> Q8_0, shape = {2048, 512}\n","INFO:hf-to-gguf:blk.4.attn_output.weight,    torch.float32 --> Q8_0, shape = {2048, 2048}\n","INFO:hf-to-gguf:blk.4.attn_q.weight,         torch.float32 --> Q8_0, shape = {2048, 2048}\n","INFO:hf-to-gguf:blk.4.attn_v.weight,         torch.float32 --> Q8_0, shape = {2048, 512}\n","INFO:hf-to-gguf:blk.5.attn_norm.weight,      torch.float32 --> F32, shape = {2048}\n","INFO:hf-to-gguf:blk.5.ffn_down.weight,       torch.float32 --> Q8_0, shape = {8192, 2048}\n","INFO:hf-to-gguf:blk.5.ffn_gate.weight,       torch.float32 --> Q8_0, shape = {2048, 8192}\n","INFO:hf-to-gguf:blk.5.ffn_up.weight,         torch.float32 --> Q8_0, shape = {2048, 8192}\n","INFO:hf-to-gguf:blk.5.ffn_norm.weight,       torch.float32 --> F32, shape = {2048}\n","INFO:hf-to-gguf:blk.5.attn_k.weight,         torch.float32 --> Q8_0, shape = {2048, 512}\n","INFO:hf-to-gguf:blk.5.attn_output.weight,    torch.float32 --> Q8_0, shape = {2048, 2048}\n","INFO:hf-to-gguf:blk.5.attn_q.weight,         torch.float32 --> Q8_0, shape = {2048, 2048}\n","INFO:hf-to-gguf:blk.5.attn_v.weight,         torch.float32 --> Q8_0, shape = {2048, 512}\n","INFO:hf-to-gguf:blk.6.attn_norm.weight,      torch.float32 --> F32, shape = {2048}\n","INFO:hf-to-gguf:blk.6.ffn_down.weight,       torch.float32 --> Q8_0, shape = {8192, 2048}\n","INFO:hf-to-gguf:blk.6.ffn_gate.weight,       torch.float32 --> Q8_0, shape = {2048, 8192}\n","INFO:hf-to-gguf:blk.6.ffn_up.weight,         torch.float32 --> Q8_0, shape = {2048, 8192}\n","INFO:hf-to-gguf:blk.6.ffn_norm.weight,       torch.float32 --> F32, shape = {2048}\n","INFO:hf-to-gguf:blk.6.attn_k.weight,         torch.float32 --> Q8_0, shape = {2048, 512}\n","INFO:hf-to-gguf:blk.6.attn_output.weight,    torch.float32 --> Q8_0, shape = {2048, 2048}\n","INFO:hf-to-gguf:blk.6.attn_q.weight,         torch.float32 --> Q8_0, shape = {2048, 2048}\n","INFO:hf-to-gguf:blk.6.attn_v.weight,         torch.float32 --> Q8_0, shape = {2048, 512}\n","INFO:hf-to-gguf:blk.7.attn_norm.weight,      torch.float32 --> F32, shape = {2048}\n","INFO:hf-to-gguf:blk.7.ffn_down.weight,       torch.float32 --> Q8_0, shape = {8192, 2048}\n","INFO:hf-to-gguf:blk.7.ffn_gate.weight,       torch.float32 --> Q8_0, shape = {2048, 8192}\n","INFO:hf-to-gguf:blk.7.ffn_up.weight,         torch.float32 --> Q8_0, shape = {2048, 8192}\n","INFO:hf-to-gguf:blk.7.ffn_norm.weight,       torch.float32 --> F32, shape = {2048}\n","INFO:hf-to-gguf:blk.7.attn_k.weight,         torch.float32 --> Q8_0, shape = {2048, 512}\n","INFO:hf-to-gguf:blk.7.attn_output.weight,    torch.float32 --> Q8_0, shape = {2048, 2048}\n","INFO:hf-to-gguf:blk.7.attn_q.weight,         torch.float32 --> Q8_0, shape = {2048, 2048}\n","INFO:hf-to-gguf:blk.7.attn_v.weight,         torch.float32 --> Q8_0, shape = {2048, 512}\n","INFO:hf-to-gguf:blk.8.attn_norm.weight,      torch.float32 --> F32, shape = {2048}\n","INFO:hf-to-gguf:blk.8.ffn_down.weight,       torch.float32 --> Q8_0, shape = {8192, 2048}\n","INFO:hf-to-gguf:blk.8.ffn_gate.weight,       torch.float32 --> Q8_0, shape = {2048, 8192}\n","INFO:hf-to-gguf:blk.8.ffn_up.weight,         torch.float32 --> Q8_0, shape = {2048, 8192}\n","INFO:hf-to-gguf:blk.8.ffn_norm.weight,       torch.float32 --> F32, shape = {2048}\n","INFO:hf-to-gguf:blk.8.attn_k.weight,         torch.float32 --> Q8_0, shape = {2048, 512}\n","INFO:hf-to-gguf:blk.8.attn_output.weight,    torch.float32 --> Q8_0, shape = {2048, 2048}\n","INFO:hf-to-gguf:blk.8.attn_q.weight,         torch.float32 --> Q8_0, shape = {2048, 2048}\n","INFO:hf-to-gguf:blk.8.attn_v.weight,         torch.float32 --> Q8_0, shape = {2048, 512}\n","INFO:hf-to-gguf:blk.9.attn_norm.weight,      torch.float32 --> F32, shape = {2048}\n","INFO:hf-to-gguf:blk.9.ffn_down.weight,       torch.float32 --> Q8_0, shape = {8192, 2048}\n","INFO:hf-to-gguf:blk.9.ffn_gate.weight,       torch.float32 --> Q8_0, shape = {2048, 8192}\n","INFO:hf-to-gguf:blk.9.ffn_up.weight,         torch.float32 --> Q8_0, shape = {2048, 8192}\n","INFO:hf-to-gguf:blk.9.ffn_norm.weight,       torch.float32 --> F32, shape = {2048}\n","INFO:hf-to-gguf:blk.9.attn_k.weight,         torch.float32 --> Q8_0, shape = {2048, 512}\n","INFO:hf-to-gguf:blk.9.attn_output.weight,    torch.float32 --> Q8_0, shape = {2048, 2048}\n","INFO:hf-to-gguf:blk.9.attn_q.weight,         torch.float32 --> Q8_0, shape = {2048, 2048}\n","INFO:hf-to-gguf:blk.9.attn_v.weight,         torch.float32 --> Q8_0, shape = {2048, 512}\n","INFO:hf-to-gguf:output_norm.weight,          torch.float32 --> F32, shape = {2048}\n","INFO:hf-to-gguf:Set meta model\n","INFO:hf-to-gguf:Set model parameters\n","INFO:hf-to-gguf:gguf: context length = 131072\n","INFO:hf-to-gguf:gguf: embedding length = 2048\n","INFO:hf-to-gguf:gguf: feed forward length = 8192\n","INFO:hf-to-gguf:gguf: head count = 32\n","INFO:hf-to-gguf:gguf: key-value head count = 8\n","INFO:hf-to-gguf:gguf: rope theta = 500000.0\n","INFO:hf-to-gguf:gguf: rms norm epsilon = 1e-05\n","INFO:hf-to-gguf:gguf: file type = 7\n","INFO:hf-to-gguf:Set model quantization version\n","INFO:hf-to-gguf:Set model tokenizer\n","2025-04-24 15:52:45.832848: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n","E0000 00:00:1745509966.259379    3310 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","E0000 00:00:1745509966.376346    3310 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2025-04-24 15:52:46.849432: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","INFO:numexpr.utils:NumExpr defaulting to 2 threads.\n","INFO:gguf.vocab:Adding 280147 merge(s).\n","INFO:gguf.vocab:Setting special token type bos to 128000\n","INFO:gguf.vocab:Setting special token type eos to 128009\n","INFO:gguf.vocab:Setting special token type pad to 128004\n","INFO:gguf.vocab:Setting chat_template to {{- bos_token }}\n","{%- if custom_tools is defined %}\n","    {%- set tools = custom_tools %}\n","{%- endif %}\n","{%- if not tools_in_user_message is defined %}\n","    {%- set tools_in_user_message = true %}\n","{%- endif %}\n","{%- if not date_string is defined %}\n","    {%- if strftime_now is defined %}\n","        {%- set date_string = strftime_now(\"%d %b %Y\") %}\n","    {%- else %}\n","        {%- set date_string = \"26 Jul 2024\" %}\n","    {%- endif %}\n","{%- endif %}\n","{%- if not tools is defined %}\n","    {%- set tools = none %}\n","{%- endif %}\n","\n","{#- This block extracts the system message, so we can slot it into the right place. #}\n","{%- if messages[0]['role'] == 'system' %}\n","    {%- set system_message = messages[0]['content']|trim %}\n","    {%- set messages = messages[1:] %}\n","{%- else %}\n","    {%- set system_message = \"\" %}\n","{%- endif %}\n","\n","{#- System message #}\n","{{- \"<|start_header_id|>system<|end_header_id|>\\n\\n\" }}\n","{%- if tools is not none %}\n","    {{- \"Environment: ipython\\n\" }}\n","{%- endif %}\n","{{- \"Cutting Knowledge Date: December 2023\\n\" }}\n","{{- \"Today Date: \" + date_string + \"\\n\\n\" }}\n","{%- if tools is not none and not tools_in_user_message %}\n","    {{- \"You have access to the following functions. To call a function, please respond with JSON for a function call.\" }}\n","    {{- 'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.' }}\n","    {{- \"Do not use variables.\\n\\n\" }}\n","    {%- for t in tools %}\n","        {{- t | tojson(indent=4) }}\n","        {{- \"\\n\\n\" }}\n","    {%- endfor %}\n","{%- endif %}\n","{{- system_message }}\n","{{- \"<|eot_id|>\" }}\n","\n","{#- Custom tools are passed in a user message with some extra guidance #}\n","{%- if tools_in_user_message and not tools is none %}\n","    {#- Extract the first user message so we can plug it in here #}\n","    {%- if messages | length != 0 %}\n","        {%- set first_user_message = messages[0]['content']|trim %}\n","        {%- set messages = messages[1:] %}\n","    {%- else %}\n","        {{- raise_exception(\"Cannot put tools in the first user message when there's no first user message!\") }}\n","{%- endif %}\n","    {{- '<|start_header_id|>user<|end_header_id|>\\n\\n' -}}\n","    {{- \"Given the following functions, please respond with a JSON for a function call \" }}\n","    {{- \"with its proper arguments that best answers the given prompt.\\n\\n\" }}\n","    {{- 'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.' }}\n","    {{- \"Do not use variables.\\n\\n\" }}\n","    {%- for t in tools %}\n","        {{- t | tojson(indent=4) }}\n","        {{- \"\\n\\n\" }}\n","    {%- endfor %}\n","    {{- first_user_message + \"<|eot_id|>\"}}\n","{%- endif %}\n","\n","{%- for message in messages %}\n","    {%- if not (message.role == 'ipython' or message.role == 'tool' or 'tool_calls' in message) %}\n","        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' }}\n","    {%- elif 'tool_calls' in message %}\n","        {%- if not message.tool_calls|length == 1 %}\n","            {{- raise_exception(\"This model only supports single tool-calls at once!\") }}\n","        {%- endif %}\n","        {%- set tool_call = message.tool_calls[0].function %}\n","        {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' -}}\n","        {{- '{\"name\": \"' + tool_call.name + '\", ' }}\n","        {{- '\"parameters\": ' }}\n","        {{- tool_call.arguments | tojson }}\n","        {{- \"}\" }}\n","        {{- \"<|eot_id|>\" }}\n","    {%- elif message.role == \"tool\" or message.role == \"ipython\" %}\n","        {{- \"<|start_header_id|>ipython<|end_header_id|>\\n\\n\" }}\n","        {%- if message.content is mapping or message.content is iterable %}\n","            {{- message.content | tojson }}\n","        {%- else %}\n","            {{- message.content }}\n","        {%- endif %}\n","        {{- \"<|eot_id|>\" }}\n","    {%- endif %}\n","{%- endfor %}\n","{%- if add_generation_prompt %}\n","    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n","{%- endif %}\n","\n","INFO:gguf.gguf_writer:Writing the following files:\n","INFO:gguf.gguf_writer:/content/llama2-q8_0.gguf: n_tensors = 147, total_size = 1.3G\n","Writing: 100% 1.31G/1.31G [01:35<00:00, 13.8Mbyte/s]\n","INFO:hf-to-gguf:Model successfully exported to /content/llama2-q8_0.gguf\n"]}]},{"cell_type":"code","source":["from google.colab import files\n","files.download('/content/llama2-q8_0.gguf')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":34},"id":"mBtzqt7-YF63","executionInfo":{"status":"ok","timestamp":1745510101255,"user_tz":-330,"elapsed":21,"user":{"displayName":"it21264634 Sujitha.S","userId":"04194498455262512245"}},"outputId":"52c8c659-ec09-437a-90d1-5783971c3406"},"execution_count":19,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["download(\"download_ab394783-7ee9-4c2b-90e0-0222223575e8\", \"llama2-q8_0.gguf\", 1321082528)"]},"metadata":{}}]},{"cell_type":"code","source":["!cp /content/llama2-q4_0.gguf /content/drive/MyDrive/\n"],"metadata":{"id":"Q2kWjL5YYJO6"},"execution_count":null,"outputs":[]}]}